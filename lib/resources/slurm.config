/*
 * SLURM execution profile template
 * Use: nextflow run main.nf -c slurm.config -profile slurm
 */

params {
    slurmPartition = null
    slurmAccount = null
    slurmQos = null
    slurmGpu = null
}

profiles {

  slurm {

    process.executor = 'slurm'

    executor {
      name            = 'slurm'
      queueSize       = 100            // max concurrently SUBMITTED jobs (not total tasks)
      pollInterval    = '20 sec'
      submitRateLimit = '10/second'    // slow down bursts on busy schedulers
    }

    process {

      // Nextflow maps `queue` to the scheduler partition concept
      queue  = null

      // Retries due to transient file system hiccups and flaky nodes
      errorStrategy = { (task.exitStatus in [143,137] ? 'retry' : 'terminate') } // SIGTERM/OOM-kill-ish cases
      maxRetries    = 2
      maxErrors     = 20

      // If cluster hates job floods, also cap local task fan-out:
      // maxForks = 8   // per-process cap (optional)

      // Do not duplicate settings that you already set via directives
      // e.g. do not specify `--partition=...` here if you use directive `queue = ...`
      clusterOptions = []

      // Nextflow will still capture stdout/stderr, this is for SLURM log files:
      clusterOptions = (clusterOptions + [
        '--output=slurm-%x-%j.out',
        '--error=slurm-%x-%j.err'
      ])


      if( params.slurmAccount )
        clusterOptions = (clusterOptions + ["--account=${params.slurmAccount}"])
      if( params.slurmQos )
        clusterOptions = (clusterOptions + ["--qos=${params.slurmQos}"])
      if( params.slurmPartition )
        clusterOptions = (clusterOptions + ["--partition=${params.slurmPartition}"])
      if( params.slurmGpu )
        clusterOptions = (clusterOptions + ["--gres=gpu:${params.slurmGpu}"])
        runOptions = "--nv"
    }
  }
}